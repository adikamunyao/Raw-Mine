{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5836a41-409d-49a7-9cbe-2007e1f9038a",
   "metadata": {},
   "source": [
    "# <center>Themes Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a5dc2d-6203-4b17-9e1d-dca92cc6fd22",
   "metadata": {},
   "source": [
    "#### Overview:\n",
    "<p>The proposed method involves using KeyBERT, a state-of-the-art keyword extraction tool based on transformer models like BERT. KeyBERT excels in extracting semantically relevant keywords and themes from large or complex text datasets. This method is chosen for its context-aware extraction, efficiency, and ability to handle diverse and nuanced language used in academic or policy-related texts.</p>\n",
    "\n",
    "##### Why KeyBERT is the Best Choice:\n",
    "\n",
    "`Contextual Understanding:`\n",
    "\n",
    "KeyBERT leverages BERT and other transformer-based models, which have been pre-trained on vast corpora to understand words in context, unlike traditional methods like TF-IDF or Word2Vec.\n",
    "This makes it particularly useful for identifying themes that are deeply tied to the meaning of the words in the document rather than just their frequency.\n",
    "In the context of ocean governance, KeyBERT can accurately identify complex terms and ideas related to international cooperation, policy frameworks, and environmental governance that might be overlooked by simpler methods.\n",
    "\n",
    "`Theme Identification and Semantic Relevance:`\n",
    "\n",
    "KeyBERT extracts the most relevant keywords by calculating the semantic similarity between words and the document. This allows for the identification of key concepts and themes (e.g., state actors, international law, environmental co-operation) without manually tagging or categorizing.\n",
    "It is especially helpful when extracting abstract themes from documents that discuss high-level concepts like global environmental governance or policy analysis.\n",
    "\n",
    "`Flexibility with Multilingual and Domain-Specific Texts:`\n",
    "\n",
    "KeyBERT is flexible enough to handle multilingual datasets, which may be useful for analyzing international documents related to governance, treaties, and policies that might use multiple languages.\n",
    "It can also be easily adapted to handle domain-specific jargon and technical terms by using appropriate transformer models tailored for the domain (e.g., domain-specific BERT models).\n",
    "\n",
    "`Ease of Use and Minimal Setup:`\n",
    "\n",
    "KeyBERT is easy to implement with minimal coding required. It provides an intuitive interface to extract keywords or themes, which is ideal for a researcher or practitioner with limited machine learning background.\n",
    "The method requires no labeled data or complex training processes, as it is an unsupervised approach. This makes it highly practical for quick analysis of large datasets.\n",
    "\n",
    "`Maximal Marginal Relevance (MMR) for Diversity:`\n",
    "\n",
    "The MMR feature in KeyBERT allows for a balance between relevance and diversity in the keywords. This feature is valuable for ensuring that the extracted themes do not become repetitive and that the set of keywords represents a broad spectrum of ideas related to the subject.\n",
    "This is particularly useful in documents where you might want to capture the full range of themes (e.g., different aspects of ocean governance like international treaties, climate change adaptation, and public-private partnerships)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1b94fd-fbf5-4909-91c1-136e964842e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "from keybert import KeyBERT\n",
    "import pandas as pd\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f463d8a5-3783-455c-acad-a8fed9286887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned sections\n",
    "file_path = '../Data/Extracted Sections/Attribute_Papers (1).xlsx'\n",
    "data = pd.read_excel(file_path, sheet_name=\"Cleaned_Extracted_Sections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df9511e-93a9-442b-ada6-2a4238f6a6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing NaN values with empty strings \n",
    "data.fillna('', inplace=True)\n",
    "# preview the df\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71fded0-bbce-432c-8364-165027732152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the function to remove list brackets and join words into a single string\n",
    "def remove_list_brackets(input_str):\n",
    "    \"\"\"\n",
    "    Function to remove list brackets and join words into a continuous string.\n",
    "    Assumes input is a string representation of a list of lists.\n",
    "    \"\"\"\n",
    "    # Convert string to list using ast.literal_eval\n",
    "    data = ast.literal_eval(input_str)\n",
    "    \n",
    "    # Flatten the list and join words into a single string\n",
    "    flat_list = [word for sublist in data for word in sublist]\n",
    "    \n",
    "    # Join the words into a single string separated by spaces\n",
    "    return ' '.join(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12046657-c059-42a4-b68a-8c4020efd94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the column\n",
    "data['Cleaned_Introduction'] = data['Cleaned_Introduction'].apply(remove_list_brackets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a6137f-20b2-4b35-8f27-e038876a37a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview cleaned data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce57779e-abfe-4a16-b434-c27d7bd9ee61",
   "metadata": {},
   "source": [
    "#### KeyBERT Workflow:\n",
    "\n",
    "$Input:$\n",
    ">Provide KeyBERT with raw text (e.g., paragraphs from research papers, policy documents, or other content related to ocean governance).\n",
    "\n",
    "$Keyword Extraction:$\n",
    ">Use KeyBERT’s extract_keywords() method to generate semantically relevant keywords. The model uses transformer-based embeddings to find the most important keywords.\n",
    "\n",
    "$Post-processing:$\n",
    ">Optionally, use Maximal Marginal Relevance (MMR) to adjust the diversity of the extracted keywords.\n",
    "Review and refine the extracted keywords to capture the core themes, ensuring that they align with the specific aspects of ocean governance you wish to focus on.\n",
    "\n",
    "$Theme Identification:$\n",
    ">After extracting the keywords, you can cluster similar keywords together or manually group them to identify abstract themes or topics (e.g., marine conservation, policy frameworks, climate change mitigation, etc.).\n",
    "\n",
    "#### Advantages of KeyBERT:\n",
    "* Deep Semantic Understanding: Captures the meaning of words in context rather than relying on frequency or superficial relationships.\n",
    "* Unsupervised Learning: Does not require labeled data, making it ideal for quick analysis and explorations of large text datasets.\n",
    "* High Accuracy: Accurate identification of themes, especially when working with complex, technical, or domain-specific texts (like ocean governance).\n",
    "* Flexible and Scalable: Suitable for both small datasets (e.g., individual reports) and large datasets (e.g., collections of academic papers or policy documents).\n",
    "* Minimal Computational Overhead: The transformer models used by KeyBERT are pre-trained, meaning that you don’t need to train a model from scratch, saving significant time and computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfea3916-91f4-4232-8f5b-6214f5b5bf51",
   "metadata": {},
   "source": [
    "* #### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f10e3c-d374-4d20-ae68-3a7bd46da266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract keywords from a document\n",
    "def keywords_extractor(doc):\n",
    "    kw_model = KeyBERT()\n",
    "    keywords = kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 1), stop_words=None)\n",
    "    return [keyword[0] for keyword in keywords]  # Extract only the keyword part from the tuples\n",
    "\n",
    "# Function to extract keyphrases from a document\n",
    "def keyphrases_extractor(doc):\n",
    "    kw_model = KeyBERT()\n",
    "    keywords = kw_model.extract_keywords(doc, keyphrase_ngram_range=(2, 5), stop_words='english',\n",
    "                                     use_mmr=True, diversity=0.7)\n",
    "    return [keyword[0] for keyword in keywords]  # Extract only the keyword part from the tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da8e753-4b2e-4c3b-bdcb-d06d282bd217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to highlight keyphrases in a document using KeyBERT's highlighting\n",
    "def keywords_highlight(doc):\n",
    "    kw_model = KeyBERT()\n",
    "    \n",
    "    # Extract keywords with highlighting enabled (returns highlighted text)\n",
    "    keywords = kw_model.extract_keywords(doc, highlight=True)\n",
    "    \n",
    "    # The highlighted text is returned directly by KeyBERT, so just return that\n",
    "    highlighted_text = doc  # KeyBERT has already applied the highlighting internally\n",
    "    \n",
    "    # Return the highlighted text and the list of extracted keywords\n",
    "    return highlighted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e7a284-0677-4984-ac8c-a91bd7bf09f2",
   "metadata": {},
   "source": [
    "* #### Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab658730-b2d4-4dff-9c22-2aaf660d81ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each document and create a new column\n",
    "data['Cleaned_Abstract_keywords'] = data['Cleaned_Abstract'].apply(keywords_extractor)\n",
    "data['Cleaned_Abstract_keyphrases'] = data['Cleaned_Abstract'].apply(keyphrases_extractor)\n",
    "# data['Cleaned_Abstract_highlights'] = data['Cleaned_Abstract'].apply(keywords_highlight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c51f85-4040-416f-a2da-051db0cd7d53",
   "metadata": {},
   "source": [
    "* #### Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843e3636-cfe2-485e-822e-0d5bc337a48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each document and create a new column 'keywords'\n",
    "data['Cleaned_Introduction_keywords'] = data['Cleaned_Introduction'].apply(keywords_extractor)\n",
    "data['Cleaned_Introduction_keyphrases'] = data['Cleaned_Introduction'].apply(keyphrases_extractor)\n",
    "# data['Cleaned_Introduction_highlights'] = data['Cleaned_Introduction'].apply(keywords_highlight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfe4415-006e-4b3d-b96e-2aac04f7f718",
   "metadata": {},
   "source": [
    "* #### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a13a4-d289-49fe-a185-57f3e02fa059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each document and create a new column 'keywords'\n",
    "data['Cleaned_Conclusion_keywords'] = data['Cleaned_Conclusion'].apply(keywords_extractor)\n",
    "data['Cleaned_Conclusion_keyphrases'] = data['Cleaned_Conclusion'].apply(keyphrases_extractor)\n",
    "# data['Cleaned_Conclusion_highlights'] = data['Cleaned_Conclusion'].apply(keywords_highlight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd68e16-f28c-48b0-9509-8cf7c0214b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[[\"File Name\", \"Cleaned_Introduction_keywords\", \"Cleaned_Introduction_keyphrases\", \"Cleaned_Abstract_keywords\", \"Cleaned_Abstract_keyphrases\", \"Cleaned_Conclusion_keywords\", \"Cleaned_Conclusion_keyphrases\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf696a-2082-4ac7-b34e-235af5a6f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save Spatial df sheet\n",
    "# with pd.ExcelWriter(file_path, mode='a') as writer:\n",
    "#     df.to_excel(writer, sheet_name='Keypharses_Extracted_Sections', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483887f5-fc24-40d4-9bf4-663fc2152257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned sections\n",
    "file_path = '../Data/Extracted Sections/Attribute_Papers (1).xlsx'\n",
    "data = pd.read_excel(file_path, sheet_name=\"Keypharses_Extracted_Sections\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b081e54-d1e1-4533-9c00-e61b497298c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b526bf-0b0a-4126-9f4f-8f3466425d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing NaN values with empty strings \n",
    "df.fillna('', inplace=True)\n",
    "# preview the df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d338004-2087-4658-915d-82ab04a1b398",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create the function to remove list brackets and join words into a single string\n",
    "def remove_list_brackets(input_str):\n",
    "    \"\"\"\n",
    "    Function to remove list brackets and join words into a continuous string.\n",
    "    Assumes input is a string representation of a list of lists.\n",
    "    \"\"\"\n",
    "    # Convert string to list using ast.literal_eval\n",
    "    data = ast.literal_eval(input_str)\n",
    "    \n",
    "    # Flatten the list and join words into a single string\n",
    "    flat_list = [word for sublist in data for word in sublist]\n",
    "    \n",
    "    # Join the words into a single string separated by spaces\n",
    "    return ','.join(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095534a2-9864-4e9b-8832-f7f8f03eaf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the keywords from the three sections into a new column\n",
    "df['Keywords'] = (\n",
    "    df['Cleaned_Introduction_keywords'] + ', ' +\n",
    "    df['Cleaned_Abstract_keywords'] + ', ' +\n",
    "    df['Cleaned_Conclusion_keywords']\n",
    ")\n",
    "\n",
    "# Remove any extra spaces or commas\n",
    "df['Keywords'] = df['Keywords'].str.replace(',\\s*,', ',', regex=True).str.strip(', ')\n",
    "# Apply the function to the column\n",
    "df['Keywords'] = df['Keywords'].apply(remove_list_brackets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ebd917-dc93-469a-8ecb-2a887f3bbb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the keyphrases from the three sections into a new column\n",
    "df['Keyphrases'] = (\n",
    "    df['Cleaned_Introduction_keyphrases'] + ', ' +\n",
    "    df['Cleaned_Abstract_keyphrases'] + ', ' +\n",
    "    df['Cleaned_Conclusion_keyphrases']\n",
    ")\n",
    "\n",
    "# Remove any extra spaces or commas\n",
    "df['Keyphrases'] = df['Keyphrases'].str.replace(',\\s*,', ',', regex=True).str.strip(', ')\n",
    "\n",
    "# Convert to set to remove duplicates and convert it back to a comma-separated string\n",
    "df['Keyphrases'] = df['Keyphrases'].apply(lambda x: ', '.join(set(x.split(', '))))\n",
    "\n",
    "# Removing empty brackets '[]' from all columns\n",
    "df['Keyphrases'] = df['Keyphrases'].apply(lambda x: x.replace(\"[]\", \"\"))\n",
    "\n",
    "# Function to clean the list-like strings and join with commas\n",
    "def clean_phrases(phrase_str):\n",
    "    # Remove the list structure and extra quotes, then join with commas\n",
    "    return ', '.join([phrase.strip(\" '\") for phrase in phrase_str.replace(\"['\", \"\").replace(\"']\", \"\").split(',')])\n",
    "\n",
    "# Apply the cleaning function to the column\n",
    "df['Keyphrases'] = df['Keyphrases'].apply(clean_phrases)\n",
    "\n",
    "\n",
    "# Display the resulting DataFrame with the new column\n",
    "df[['File Name', 'Keyphrases']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb42c4e-58a5-4a3a-ae86-8a229fa38372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()\n",
    "# df = df[[\"File Name\", \"Keywords\", \"Keyphrases\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd97fe97-be36-4f6e-bc62-c32587ad5ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save Spatial df sheet\n",
    "# with pd.ExcelWriter(file_path, mode='a') as writer:\n",
    "#     df.to_excel(writer, sheet_name='Key_Sections_Extracted', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8832c7c1-8b59-4648-89a2-9e7d8c4e791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned sections\n",
    "file_path = '../Data/Extracted Sections/Attribute_Papers (1).xlsx'\n",
    "data = pd.read_excel(file_path, sheet_name=\"Key_Sections_Extracted\")\n",
    "\n",
    "# Replacing NaN values with empty strings \n",
    "data.fillna('', inplace=True)\n",
    "# preview the df\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902202a9-9942-4f05-be36-417b409b62c4",
   "metadata": {},
   "source": [
    "### Key Themes Identified:\n",
    "\n",
    "1. Global Governance and Policy Coordination\n",
    "* `Keywords:` governance, accountability, organizations, intergovernmental, regimes, IGOs, policymaking, stakeholders, institutions, diplomats, governments, sanctions, international, multilateral, coordination, alliances, partnerships\n",
    "* $Focus:$ This theme focuses on the broader global governance structures, mechanisms, and processes that coordinate and enforce ocean-related policies, often at the international and intergovernmental levels.\n",
    "2. Institutional Design and Organizational Structures\n",
    "* `Keywords:` institutional, organizations, bureaucracies, secretariat, bureaucratic, regulatory, institutionalization, organizational, decentralization, authority\n",
    "* $Focus:$ It highlights the formal structures and institutional design involved in ocean governance, including organizations and their regulatory frameworks, decision-making bodies, and authority.\n",
    "3. Sustainability and Environmental Management\n",
    "* `Keywords:` sustainability, sustainable, environmental, ecosystems, biodiversity, conservation, reforestation, pollution, resource depletion, socio-economic, policies, initiatives\n",
    "* $Focus:$ This theme involves the management and conservation of marine environments, emphasizing sustainable practices and policies aimed at protecting ecosystems, biodiversity, and preventing pollution.\n",
    "4. Climate Change and Adaptation\n",
    "* `Keywords:` climate, adaptation, global, mainstreaming, politicization, environmental risks\n",
    "* $Focus:$ This theme focuses on the intersection of climate change with ocean governance, particularly in terms of adaptation strategies, climate mitigation efforts, and addressing environmental risks that impact marine ecosystems.\n",
    "5. Scientific Research and Knowledge Creation\n",
    "* `Keywords:` research, study, scholars, academics, researcher, journal, academic, discipline, examination, inquiry, findings, literature, methodology, survey, review, data, analysis\n",
    "* $Focus:$ This theme involves the creation and dissemination of scientific knowledge related to the oceans, covering research efforts, methodologies, and findings that inform policy and governance decisions.\n",
    "6. Private Sector, Non-State Actors, and Stakeholder Engagement\n",
    "* `Keywords:` private sector, nongovernmental, NGO, deliberation, initiatives, transnational, stakeholder\n",
    "* $Focus:$ This theme examines the involvement of non-state actors (e.g., NGOs, businesses, civil society) in ocean governance, and their influence in shaping policies, regulations, and actions through advocacy, initiatives, and partnerships.\n",
    "7. Legal Frameworks and Norms\n",
    "* `Keywords:` legal norms, treaties, regulatory, sanctions, law, internationalization, norms, policies\n",
    "* $Focus:$ This theme focuses on the legal aspects of ocean governance, including international treaties, regulatory frameworks, legal norms, and enforcement mechanisms that guide actions and compliance in marine governance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81738b6b-d915-4392-b873-0a03f3e2ed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the keyword-theme mapping based on the previous categorization\n",
    "keyword_theme_mapping = {\n",
    "    'Global Governance and Policy Coordination': [\n",
    "        'governance', 'accountability', 'organizations', 'intergovernmental', 'regimes', 'igos', 'policymaking', 'stakeholders', \n",
    "        'institutions', 'diplomats', 'governments', 'sanctions', 'international', 'multilateral', 'coordination', 'alliances', 'partnerships'\n",
    "    ],\n",
    "    \n",
    "    'Institutional Design and Organizational Structures': [\n",
    "        'institutional', 'organizations', 'bureaucracies', 'secretariat', 'bureaucratic', 'regulatory', 'institutionalization', \n",
    "        'organizational', 'decentralization', 'authority'\n",
    "    ],\n",
    "    \n",
    "    'Sustainability and Environmental Management': [\n",
    "        'sustainability', 'sustainable', 'environmental', 'ecosystems', 'biodiversity', 'conservation', 'reforestation', \n",
    "        'pollution', 'resource depletion', 'socioeconomic', 'policies', 'initiatives'\n",
    "    ],\n",
    "    \n",
    "    'Climate Change and Adaptation': [\n",
    "        'climate', 'adaptation', 'global', 'mainstreaming', 'politicization', 'environmental risks'\n",
    "    ],\n",
    "    \n",
    "    'Scientific Research and Knowledge Creation': [\n",
    "        'research', 'study', 'scholars', 'academics', 'researcher', 'journal', 'academic', 'discipline', 'examination', \n",
    "        'inquiry', 'findings', 'literature', 'methodology', 'survey', 'review', 'data', 'analysis'\n",
    "    ],\n",
    "    \n",
    "    'Private Sector, Non-State Actors, and Stakeholder Engagement': [\n",
    "        'private sector', 'nongovernmental', 'ngo', 'deliberation', 'initiatives', 'transnational', 'stakeholder'\n",
    "    ],\n",
    "    \n",
    "    'Legal Frameworks and Norms': [\n",
    "        'legal norms', 'treaties', 'regulatory', 'sanctions', 'law', 'internationalization', 'norms', 'policies'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Function to map keyphrases to themes based on the keywords\n",
    "def map_themes(keyphrase):\n",
    "    themes = []\n",
    "    \n",
    "    # Iterate through each theme and check if any keyword from that theme is in the keyphrase\n",
    "    for theme, theme_keywords in keyword_theme_mapping.items():\n",
    "        if any(keyword in keyphrase.lower() for keyword in theme_keywords):\n",
    "            themes.append(theme)\n",
    "    \n",
    "    return ', '.join(themes) if themes else 'No Theme'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21935e3d-23e6-4547-a10f-2a0eaefdec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the keyphrases column\n",
    "data['Themes'] = data['Keyphrases'].apply(map_themes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58ead32-2bab-468c-a72f-e688b83dc133",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[[\"File Name\", \"Themes\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df8e9ae-d2b4-4f5a-acbb-8b5b9d3378d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae4ae97-a62c-4d25-962a-af6c930862c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save Spatial df sheet\n",
    "# with pd.ExcelWriter(file_path, mode='a') as writer:\n",
    "#     df.to_excel(writer, sheet_name='Identified Themes', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a3722-207e-4850-a386-4a755c2b3b54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
