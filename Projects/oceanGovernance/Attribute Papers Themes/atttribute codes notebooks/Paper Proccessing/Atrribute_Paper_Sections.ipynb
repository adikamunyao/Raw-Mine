{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d624dc80-ed1b-486a-b354-6baae5496c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-community langchain-core\n",
    "# !pip install pymupdf\n",
    "# %pip install -qU pypdf\n",
    "# %pip install PyMuPDF pdfplumber pandas spacy regex\n",
    "# %pip install PyPDF2 pandas os\n",
    "# !pip install pdfplumber\n",
    "# !!python -m spacy download en_core_web_lg "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c63743-d621-405f-92dd-f34d9a4a6e45",
   "metadata": {},
   "source": [
    "## Tools and Libraries Neccessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49e75cd9-b770-42f5-8e09-1417e80f5703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libaries\n",
    "import os\n",
    "import spacy\n",
    "import gensim\n",
    "from string import punctuation\n",
    "from pypdf import PdfReader\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pdfminer.high_level import extract_text\n",
    "from langchain_community.document_loaders import PyMuPDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a867cd2-a86f-4ded-9ba7-1844c2a509fb",
   "metadata": {},
   "source": [
    "* **PyPDF2:** This library helps to read the text content from PDF files.\n",
    "* **Pandas:** Used to create a DataFrame to store your extracted data.\n",
    "* **os:** Used to interact with the file system and list files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5a9b6d-c4bd-406e-a2ac-8c33050c47dc",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84b9316d-134a-4620-9ed6-ec6b4e78b7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total journas are 78'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the path to your folder containing PDF files\n",
    "pdf_folder_path = '../Data/Attribute_Papers/'\n",
    "\n",
    "# List all the PDF files in the folder\n",
    "pdf_files = [f for f in os.listdir(pdf_folder_path) if f.endswith('.pdf')]\n",
    "f\"Total journas are {len(pdf_files)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a4f56c3-ffc8-41b6-9cd2-ee17fa3ad629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['International Organizations and Implementation. Enforcers_Managers_ Authorities.pdf',\n",
       " 'Critical Choices_The United Nations_Networks and the Future of Global Governance.pdf',\n",
       " 'Organizational Progeny. Why Governments are Losing Control over the Proliferating Structures of Global Governance.pdf',\n",
       " 'The Politics of International Environmental Management.pdf',\n",
       " 'New Alliances in Global Environmental Governance_ Hw intergovernmental treaty secretariat interact.pdf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the journals\n",
    "pdf_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53acb99-efec-4ea0-8a0d-712f439a3e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83bfd13c-d0e8-44e5-a56f-57836b903582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.043*\"the\" + 0.041*\",\" + 0.025*\"of\" + 0.017*\".\" + 0.014*\"and\"')\n",
      "(1, '0.036*\",\" + 0.028*\"the\" + 0.018*\"of\" + 0.015*\".\" + 0.011*\"and\"')\n",
      "(2, '0.059*\",\" + 0.059*\"the\" + 0.030*\"of\" + 0.029*\".\" + 0.024*\"and\"')\n"
     ]
    }
   ],
   "source": [
    "# Extract text from PDF (using pdfminer)\n",
    "pdf_text = extract_text('../Data/Attribute_Papers/“Privatisation_’ in the United Nations system_.pdf')\n",
    "\n",
    "# Tokenize text\n",
    "tokens = word_tokenize(pdf_text.lower())\n",
    "\n",
    "# Create a dictionary and corpus for LDA model\n",
    "dictionary = corpora.Dictionary([tokens])\n",
    "corpus = [dictionary.doc2bow(text) for text in [tokens]]\n",
    "\n",
    "# Apply LDA model to find topics\n",
    "lda_model = gensim.models.LdaMulticore(corpus, num_topics=3, id2word=dictionary, passes=2)\n",
    "\n",
    "# Print the themes/topics\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a736337-5386-4797-b9d7-a2c08afb291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pdfminer.high_level import extract_text\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "from gensim import corpora\n",
    "import gensim\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cf9ca96-ea0d-4e53-a887-b8234bd3e149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/milo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "The PDF <_io.BufferedReader name='../Data/Attribute_Papers/The complexification of the United Nations system.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "download('stopwords')\n",
    "\n",
    "# Define the path to your directory containing the PDF files\n",
    "pdf_dir = '../Data/Attribute_Papers/'\n",
    "\n",
    "# Define stopwords list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function for preprocessing the text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text and convert it to lowercase\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords and punctuation\n",
    "    tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
    "    \n",
    "    # Optional: Lemmatization can be added here (e.g., using nltk.stem or spacy)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Step 1: Process each PDF file individually\n",
    "for filename in os.listdir(pdf_dir):\n",
    "    if filename.endswith(\".pdf\"):  # Process only PDF files\n",
    "        pdf_path = os.path.join(pdf_dir, filename)\n",
    "        \n",
    "        # Extract text from the PDF file\n",
    "        pdf_text = extract_text(pdf_path)\n",
    "        \n",
    "        # Step 2: Preprocess the text (tokenization, stopword removal, punctuation removal)\n",
    "        tokens = preprocess_text(pdf_text)\n",
    "        \n",
    "        # Step 3: Create a dictionary and corpus for the LDA model\n",
    "        dictionary = corpora.Dictionary([tokens])  # Create a dictionary for the tokens in this paper\n",
    "        corpus = [dictionary.doc2bow(tokens)]  # Create the bag-of-words for this paper\n",
    "        \n",
    "        # Step 4: Apply LDA model to extract themes (topics)\n",
    "        lda_model = gensim.models.LdaMulticore(corpus, num_topics=10, id2word=dictionary, passes=2)\n",
    "        \n",
    "        # Step 5: Extract and format the topics for the current paper\n",
    "        topics = lda_model.print_topics(num_words=5)\n",
    "        themes = [f\"Topic {i+1}: {topic[1]}\" for i, topic in enumerate(topics)]\n",
    "        \n",
    "        # Step 6: Store the filename and its associated topics in the results list\n",
    "        results.append({\"File\": filename, \"Themes\": \"; \".join(themes)})\n",
    "\n",
    "# Step 7: Create a DataFrame from the results list\n",
    "df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d7d30416-71c3-4046-84c1-354c1f824081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has been saved to 'themes_analysis1.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Display the DataFrame (or save it to a file if needed)\n",
    "# df['Themes'][0]\n",
    "\n",
    "\n",
    "# Create a DataFrame from the results list\n",
    "# df = pd.DataFrame(results)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel('themes_analysis.xlsx', index=False, engine='openpyxl')\n",
    "\n",
    "# Confirmation message\n",
    "print(\"DataFrame has been saved to 'themes_analysis1.xlsx'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c86347-c047-45a8-8248-496907cfcce4",
   "metadata": {},
   "source": [
    "## Key Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cdd0bc97-cb56-46db-b495-80be8e36878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebe746bd-2a2b-4a5c-8f3f-3f69860fccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from pdfminer.high_level import extract_text\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "from gensim import corpora\n",
    "import gensim\n",
    "import string\n",
    "from gensim.models import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "501ba2ea-4e8c-4e1c-a70d-2d16c562b222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/milo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "download('stopwords')\n",
    "\n",
    "# Load the spaCy model for Named Entity Recognition (NER)\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Increase the text length limit (example: set it to 2 million characters)\n",
    "nlp.max_length = 2000000 \n",
    "\n",
    "# Define stopwords list\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf3c2505-ae26-43ff-87fa-a647f7a2b555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PDF <_io.BufferedReader name='../Data/Attribute_Papers/The complexification of the United Nations system.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n"
     ]
    }
   ],
   "source": [
    "# Function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text and convert it to lowercase\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove stopwords and punctuation\n",
    "    tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
    "    return tokens\n",
    "\n",
    "# Function to extract named entities from text\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "# Function to create bigrams and trigrams\n",
    "def create_bigrams_trigrams(tokens):\n",
    "    bigram_model = Phrases([tokens], min_count=5, threshold=100)\n",
    "    trigram_model = Phrases(bigram_model[[tokens]], threshold=100)\n",
    "    bigrams = bigram_model[tokens]\n",
    "    trigrams = trigram_model[bigrams]\n",
    "    return bigrams, trigrams\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "# Path to your PDF directory\n",
    "pdf_dir = '../Data/Attribute_Papers/'\n",
    "\n",
    "# Process each PDF file\n",
    "for filename in os.listdir(pdf_dir):\n",
    "    if filename.endswith(\".pdf\"):  # Process only PDF files\n",
    "        pdf_path = os.path.join(pdf_dir, filename)\n",
    "        \n",
    "        # Extract text from the PDF file\n",
    "        pdf_text = extract_text(pdf_path)\n",
    "        \n",
    "        # Extract named entities from the text\n",
    "        entities = extract_entities(pdf_text)\n",
    "        \n",
    "        # Remove named entities from the text\n",
    "        cleaned_text = ' '.join([word for word in pdf_text.split() if word not in entities])\n",
    "        \n",
    "        # Preprocess the cleaned text\n",
    "        tokens = preprocess_text(cleaned_text)\n",
    "        \n",
    "        # Create bigrams and trigrams\n",
    "        bigrams, trigrams = create_bigrams_trigrams(tokens)\n",
    "        \n",
    "        # Create a dictionary and corpus for the LDA model\n",
    "        dictionary = corpora.Dictionary([bigrams])  # Use bigrams\n",
    "        corpus = [dictionary.doc2bow(bigrams)]  # Create bag-of-words for this paper\n",
    "        \n",
    "        # Apply LDA model to extract themes\n",
    "        lda_model = gensim.models.LdaMulticore(corpus, num_topics=10, id2word=dictionary, passes=5)\n",
    "        \n",
    "        # Extract and format the topics for the paper\n",
    "        topics = lda_model.print_topics(num_words=5)\n",
    "        themes = [f\"Topic {i+1}: {topic[1]}\" for i, topic in enumerate(topics)]\n",
    "        \n",
    "        # Store the filename, themes, and named entities in the results\n",
    "        results.append({\"File\": filename, \"Themes\": \"; \".join(themes), \"Entities\": \", \".join(set(entities))})\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "phrases_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0a5f1485-f2f2-4785-9b57-b207a94c3bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Topic 1: 0.005*\"ios\" + 0.003*\"implementation\" ...\n",
       "1     Topic 1: 0.016*\"networks\" + 0.010*\"global\" + 0...\n",
       "2     Topic 1: 0.002*\"international\" + 0.002*\"e\" + 0...\n",
       "3     Topic 1: 0.003*\"international\" + 0.003*\"enviro...\n",
       "4     Topic 1: 0.012*\"secretariat\" + 0.011*\"actors\" ...\n",
       "                            ...                        \n",
       "73    Topic 1: 0.006*\"environmental\" + 0.005*\"govern...\n",
       "74    Topic 1: 0.015*\"international\" + 0.012*\"organi...\n",
       "75    Topic 1: 0.017*\"environmental\" + 0.008*\"``\" + ...\n",
       "76    Topic 1: 0.009*\"governance\" + 0.009*\"ios\" + 0....\n",
       "77    Topic 1: 0.004*\"e\" + 0.003*\"secretariats\" + 0....\n",
       "Name: Themes, Length: 78, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c152a342-c2b5-4e08-9a15-cad53ac09229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save phrases_df to a different CSV file\n",
    "phrases_df.to_csv('phrases_analysis.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b7efcae-5e76-4259-94c0-b8487dad853f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/milo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/milo/anaconda3/lib/python3.12/site-packages/gensim/models/ldamodel.py:847: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  perwordbound = self.bound(chunk, subsample_ratio=subsample_ratio) / (subsample_ratio * corpus_words)\n",
      "The PDF <_io.BufferedReader name='../Data/Attribute_Papers/The complexification of the United Nations system.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models.phrases import Phrases\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the spaCy model for Named Entity Recognition (NER)\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Define stopwords list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function for text preprocessing (tokenization, removing stopwords, punctuation)\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n",
    "    tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
    "    return tokens\n",
    "\n",
    "# Function to create bigrams/trigrams\n",
    "def create_bigrams_trigrams(tokens):\n",
    "    bigram_model = Phrases([tokens], min_count=5, threshold=100)  # Create bigram model\n",
    "    trigram_model = Phrases(bigram_model[[tokens]], min_count=5, threshold=100)  # Create trigram model\n",
    "    bigrams = bigram_model[tokens]  # Apply bigrams to the text\n",
    "    trigrams = trigram_model[bigrams]  # Apply trigrams to the text\n",
    "    return trigrams\n",
    "\n",
    "# Function to process the PDF documents\n",
    "def process_pdfs(pdf_dir):\n",
    "    results = []\n",
    "    \n",
    "    for filename in os.listdir(pdf_dir):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_dir, filename)\n",
    "            \n",
    "            # Step 1: Extract text from PDF\n",
    "            pdf_text = extract_text(pdf_path)\n",
    "            \n",
    "            # Step 2: Preprocess the text (tokenization, stopword removal)\n",
    "            tokens = preprocess_text(pdf_text)\n",
    "            \n",
    "            # Step 3: Create bigrams and trigrams from the tokens\n",
    "            trigrams = create_bigrams_trigrams(tokens)\n",
    "            \n",
    "            # Step 4: Create a dictionary and corpus for Lfrom nltk import ngramsDA\n",
    "            dictionary = corpora.Dictionary([trigrams])\n",
    "            corpus = [dictionary.doc2bow(trigrams)]\n",
    "            \n",
    "            # Step 5: Apply TF-IDF model to adjust word weights\n",
    "            tfidf_model = TfidfModel(corpus)\n",
    "            corpus_tfidf = tfidf_model[corpus]\n",
    "            \n",
    "            # Step 6: Apply LDA model to extract topics\n",
    "            lda_model = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2)\n",
    "            \n",
    "            # Step 7: Extract themes (topics) from the model\n",
    "            topics = lda_model.print_topics(num_words=5)\n",
    "            themes = [f\"Topic {i+1}: {topic[1]}\" for i, topic in enumerate(topics)]\n",
    "            \n",
    "            # Step 8: Store the results (filename and extracted themes)\n",
    "            results.append({\"File\": filename, \"Themes\": \"; \".join(themes)})\n",
    "    \n",
    "    # Step 9: Convert the results into a DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "# Define the directory containing the PDFs\n",
    "pdf_dir = '../Data/Attribute_Papers/'\n",
    "\n",
    "# Process the PDFs and get the results as a DataFrame\n",
    "df2 = process_pdfs(pdf_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "21af54d9-d5be-4cab-82f7-b8f8e6eb6f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Topic 1: 0.001*\"penalize\" + 0.001*\"per-\" + 0.001*\"pean\" + 0.001*\"penal-\" + 0.001*\"pattern\"; Topic 2: 0.001*\"penalize\" + 0.001*\"per-\" + 0.001*\"pean\" + 0.001*\"penal-\" + 0.001*\"pattern\"; Topic 3: 0.001*\"penalize\" + 0.001*\"per-\" + 0.001*\"pean\" + 0.001*\"penal-\" + 0.001*\"pattern\"; Topic 4: 0.001*\"penalize\" + 0.001*\"per-\" + 0.001*\"pean\" + 0.001*\"penal-\" + 0.001*\"pattern\"; Topic 5: 0.001*\"penalize\" + 0.001*\"per-\" + 0.001*\"pean\" + 0.001*\"penal-\" + 0.001*\"pattern\"; Topic 6: 0.001*\"penalize\" + 0.001*\"per-\" + 0.001*\"pean\" + 0.001*\"penal-\" + 0.001*\"pattern\"; Topic 7: 0.001*\"penalize\" + 0.001*\"per-\" + 0.001*\"pean\" + 0.001*\"penal-\" + 0.001*\"pattern\"; Topic 8: 0.001*\"penalize\" + 0.001*\"per-\" + 0.001*\"pean\" + 0.001*\"penal-\" + 0.001*\"pattern\"; Topic 9: 0.001*\"penalize\" + 0.001*\"per-\" + 0.001*\"pean\" + 0.001*\"penal-\" + 0.001*\"pattern\"; Topic 10: 0.001*\"penalize\" + 0.001*\"per-\" + 0.001*\"pean\" + 0.001*\"penal-\" + 0.001*\"pattern\"'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[\"Themes\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a39fd18-6cad-43df-a733-a10974a01967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
